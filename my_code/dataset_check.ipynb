{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmpose.datasets.datasets.body import Mo2Cap2CocoDataset\n",
    "from mmengine import Config\n",
    "from mmengine import build_from_cfg, is_seq_of\n",
    "from mmpose.registry import DATASETS\n",
    "from mmpose.datasets.transforms.bottomup_transforms import (BottomupGetHeatmapMask, BottomupRandomAffine,\n",
    "                                  BottomupRandomChoiceResize,\n",
    "                                  BottomupRandomCrop, BottomupResize)\n",
    "from mmpose.datasets.transforms.common_transforms import (Albumentation, FilterAnnotations,\n",
    "                                GenerateTarget, GetBBoxCenterScale,\n",
    "                                PhotometricDistortion, RandomBBoxTransform,\n",
    "                                RandomFlip, RandomHalfBody, YOLOXHSVRandomAug)\n",
    "\n",
    "from mmpose.datasets.transforms.custom_transforms import EgoposeFilterAnnotations,FisheyeCropTransform\n",
    "\n",
    "from mmpose.datasets.transforms.converting import KeypointConverter, SingleHandConverter\n",
    "from mmpose.datasets.transforms.formatting import PackPoseInputs\n",
    "from mmpose.datasets.transforms.hand_transforms import HandRandomFlip\n",
    "from mmpose.datasets.transforms.loading import LoadImage\n",
    "from mmpose.datasets.transforms.mix_img_transforms import Mosaic, YOLOXMixUp\n",
    "from mmpose.datasets.transforms.pose3d_transforms import RandomFlipAroundRoot\n",
    "from mmpose.datasets.transforms.topdown_transforms import TopdownAffine\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cfg = Config.fromfile(\n",
    "\tr'C:\\Users\\user\\Documents\\GitHub\\mmpose\\my_code\\custom_config\\HMD_mo2cap2_imple_config.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Mo2Cap2CocoDataset',\n",
       " 'data_root': 'F:\\\\mo2cap2_data_half\\\\TrainSet',\n",
       " 'data_mode': 'topdown',\n",
       " 'filter_cfg': {'filter_empty_gt': False, 'min_size': 32},\n",
       " 'pipeline': [{'type': 'LoadImage'},\n",
       "  {'type': 'GetBBoxCenterScale', 'padding': 1.0},\n",
       "  {'type': 'TopdownAffine', 'input_size': (256, 256)},\n",
       "  {'type': 'GenerateTarget',\n",
       "   'encoder': {'type': 'Custom_mo2cap2_MSRAHeatmap',\n",
       "    'input_size': (256, 256),\n",
       "    'heatmap_size': (47, 47),\n",
       "    'sigma': 3}},\n",
       "  {'type': 'PackPoseInputs'}],\n",
       " 'input_size': (256, 256)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.train_dataloader\n",
    "cfg.dataset_mo2cap2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "codec = dict(\n",
    "\ttype='Custom_mo2cap2_MSRAHeatmap', input_size=(384, 384), heatmap_size=(47, 47), sigma=2)\n",
    "\n",
    "# data\n",
    "dataset_type = 'Mo2Cap2CocoDataset'\n",
    "data_mode = 'topdown'\n",
    "# data_root = r'C:\\Users\\user\\Documents\\GitHub\\mmpose\\data'\n",
    "\n",
    "# ---\n",
    "# # mo2cap2 dataset temp\n",
    "# ann_file_val = r'F:\\mo2cap2_data_temp_extracted\\TestSet'\n",
    "# ann_file_train = r'F:\\mo2cap2_data_temp_extracted\\TrainSet'\n",
    "# ---\n",
    "# # ## mo2cap2 dataset small, test small\n",
    "# ann_file_test = r'F:\\mo2cap2_data_small\\TestSet'\n",
    "# ann_file_val = r'F:\\mo2cap2_data_small\\ValSet'\n",
    "# ann_file_train = r'F:\\mo2cap2_data_small\\TrainSet'\n",
    "# ---\n",
    "# mo2cap2 dataset train middel, test all\n",
    "ann_file_test = r'F:\\mo2cap2_data_half\\TestSet'\n",
    "ann_file_val = r'F:\\mo2cap2_data_half\\ValSet'\n",
    "ann_file_train = r'F:\\mo2cap2_data_half\\TrainSet'\n",
    "# ---\n",
    "# # mo2cap2 dataset train all, test all\n",
    "# ann_file_val = r'F:\\extracted_mo2cap2_dataset\\TestSet'\n",
    "# ann_file_train = r'F:\\extracted_mo2cap2_dataset\\TrainSet'\n",
    "\n",
    "####\n",
    "# ann_file_test = r'F:\\mo2cap2_one_data\\TestSet'\n",
    "# ann_file_val = r'F:\\mo2cap2_one_data\\ValSet'\n",
    "# ann_file_train = r'F:\\mo2cap2_one_data\\TrainSet'\n",
    "###\n",
    "\n",
    "# ann_file_test = r'D:\\cross_plat\\mo2cap2_data_half\\TestSet'\n",
    "# ann_file_val = r'D:\\cross_plat\\mo2cap2_data_half\\ValSet'\n",
    "# ann_file_train = r'D:\\cross_plat\\mo2cap2_data_half\\TrainSet'\n",
    "\n",
    "\n",
    "train_pipeline = [\n",
    "\tLoadImage(),\n",
    "\tGetBBoxCenterScale(padding=1.),\n",
    "\t# dict(type='RandomFlip', direction='horizontal'), # 3d keypoints도 플립시켜야됨.\n",
    "\t# # dict(type='RandomHalfBody'),\n",
    "\t# # dict(type='RandomBBoxTransform'),\n",
    "\tTopdownAffine(input_size=codec['input_size']),\n",
    "\tGenerateTarget(encoder=codec),\n",
    "\tPackPoseInputs(),\n",
    "]\n",
    "\n",
    "val_pipeline = [\n",
    "\tLoadImage(),\n",
    "\tGetBBoxCenterScale(padding=1.),\n",
    "\tTopdownAffine(input_size=codec['input_size']),\n",
    "\tGenerateTarget(encoder=codec),\n",
    "\tPackPoseInputs(),\n",
    "]\n",
    "\n",
    "dataset_mo2cap2_train = dict(\n",
    "\t# type=dataset_type,\n",
    "\tdata_root=ann_file_train,\n",
    "\tdata_mode=data_mode,\n",
    "\tfilter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
    "\tpipeline=train_pipeline,\n",
    ")\n",
    "\n",
    "dataset_mo2cap2_val = dict(\n",
    "\t# type=dataset_type,\n",
    "\tdata_root=ann_file_test,\n",
    "\tdata_mode=data_mode,\n",
    "\tfilter_cfg=dict(filter_empty_gt=False, min_size=32),\n",
    "\t# ann_file=ann_file_val,\n",
    "\t# data_prefix=dict(img=r'C:\\Users\\user\\Documents\\GitHub\\mmpose\\data\\coco\\train2017'),\n",
    "\tpipeline=val_pipeline,\n",
    "\ttest_mode = True,\n",
    "\tinput_size = codec['input_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Mo2Cap2CocoDataset(**dataset_mo2cap2_train)\n",
    "val_dataset = Mo2Cap2CocoDataset(**dataset_mo2cap2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384, 384])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]['inputs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PoseDataSample(\n",
       "\n",
       "    META INFORMATION\n",
       "    dataset_name: 'mo2cap2'\n",
       "    img_shape: (256, 256)\n",
       "    ori_shape: (256, 256)\n",
       "    img_id: 32999\n",
       "    input_center: array([128., 128.])\n",
       "    flip_indices: [0, 4, 5, 6, 1, 2, 3, 11, 12, 13, 14, 7, 8, 9, 10]\n",
       "    input_size: (256, 256)\n",
       "    img_path: 'F:\\\\mo2cap2_data_half\\\\TrainSet\\\\mo2cap2_chunk_0032\\\\rgba\\\\mo2cap2_chunk_0032_000999.png'\n",
       "    input_scale: array([256., 256.])\n",
       "    raw_ann_info: \n",
       "        img_path: 'F:\\\\mo2cap2_data_half\\\\TrainSet\\\\mo2cap2_chunk_0032\\\\rgba\\\\mo2cap2_chunk_0032_000999.png'\n",
       "        json_path: 'F:\\\\mo2cap2_data_half\\\\TrainSet\\\\mo2cap2_chunk_0032\\\\json\\\\mo2cap2_chunk_0032_000999.json'\n",
       "        bbox: array([[  0,   0, 256, 256]])\n",
       "        keypoints: array([[[131.56540762, 195.38776544],\n",
       "                    [168.12443172, 173.36135359],\n",
       "                    [183.40560259, 129.60919845],\n",
       "                    [205.51045079,  94.66215272],\n",
       "                    [ 87.7879861 , 185.06874965],\n",
       "                    [ 72.66938765, 139.74631743],\n",
       "                    [ 67.31058039,  99.53231852],\n",
       "                    [132.06816811, 134.37554577],\n",
       "                    [130.57246254, 124.26598301],\n",
       "                    [129.41791328, 121.14242689],\n",
       "                    [131.22153159, 111.40494131],\n",
       "                    [112.16048657, 138.20456109],\n",
       "                    [114.16840506, 128.433823  ],\n",
       "                    [115.36985153, 123.41907338],\n",
       "                    [110.90877458, 115.00914632]]])\n",
       "        area: array(47700., dtype=float32)\n",
       "\n",
       "    DATA FIELDS\n",
       "    gt_instances: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "            keypoints_visible: array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "                      dtype=float32)\n",
       "            bbox_scores: array([1.], dtype=float32)\n",
       "            bbox_scales: array([[256., 256.]])\n",
       "            keypoint3d: array([[[ 0.        ,  0.        ,  0.        ],\n",
       "                        [ 0.14961515, -0.037775  ,  0.064975  ],\n",
       "                        [ 0.33139115, -0.2239104 ,  0.212347  ],\n",
       "                        [ 0.36159215, -0.390676  ,  0.019002  ],\n",
       "                        [-0.17257185,  0.001603  ,  0.022281  ],\n",
       "                        [-0.37208285, -0.1597212 ,  0.178165  ],\n",
       "                        [-0.27295785, -0.345204  ,  0.026347  ],\n",
       "                        [ 0.00254999, -0.1872039 ,  0.530604  ],\n",
       "                        [-0.02001125, -0.334304  ,  0.95986   ],\n",
       "                        [-0.05100185, -0.441761  ,  1.32902   ],\n",
       "                        [-0.01371195, -0.650796  ,  1.32261   ],\n",
       "                        [-0.19035685, -0.150136  ,  0.511406  ],\n",
       "                        [-0.27571885, -0.268896  ,  0.93822   ],\n",
       "                        [-0.34101985, -0.393247  ,  1.29961   ],\n",
       "                        [-0.44006585, -0.572762  ,  1.28635   ]]])\n",
       "            bboxes: array([[  0,   0, 256, 256]])\n",
       "            keypoints: array([[[131.56540762, 195.38776544],\n",
       "                        [168.12443172, 173.36135359],\n",
       "                        [183.40560259, 129.60919845],\n",
       "                        [205.51045079,  94.66215272],\n",
       "                        [ 87.7879861 , 185.06874965],\n",
       "                        [ 72.66938765, 139.74631743],\n",
       "                        [ 67.31058039,  99.53231852],\n",
       "                        [132.06816811, 134.37554577],\n",
       "                        [130.57246254, 124.26598301],\n",
       "                        [129.41791328, 121.14242689],\n",
       "                        [131.22153159, 111.40494131],\n",
       "                        [112.16048657, 138.20456109],\n",
       "                        [114.16840506, 128.433823  ],\n",
       "                        [115.36985153, 123.41907338],\n",
       "                        [110.90877458, 115.00914632]]])\n",
       "        ) at 0x1d52eaea970>\n",
       "    gt_fields: <PixelData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "            heatmaps: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "                \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "                \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "                \n",
       "                        ...,\n",
       "                \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "                \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "                \n",
       "                        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         ...,\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "                         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
       "        ) at 0x1d52eaea880>\n",
       "    gt_instance_labels: <InstanceData(\n",
       "        \n",
       "            META INFORMATION\n",
       "        \n",
       "            DATA FIELDS\n",
       "            hmd_info_w_noise: tensor([[[-0.0080,  0.0040, -0.0070],\n",
       "                         [ 0.3656, -0.3967,  0.0100],\n",
       "                         [-0.2720, -0.3482,  0.0353]]], dtype=torch.float64)\n",
       "            keypoint3d: tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.1496, -0.0378,  0.0650],\n",
       "                         [ 0.3314, -0.2239,  0.2123],\n",
       "                         [ 0.3616, -0.3907,  0.0190],\n",
       "                         [-0.1726,  0.0016,  0.0223],\n",
       "                         [-0.3721, -0.1597,  0.1782],\n",
       "                         [-0.2730, -0.3452,  0.0263],\n",
       "                         [ 0.0025, -0.1872,  0.5306],\n",
       "                         [-0.0200, -0.3343,  0.9599],\n",
       "                         [-0.0510, -0.4418,  1.3290],\n",
       "                         [-0.0137, -0.6508,  1.3226],\n",
       "                         [-0.1904, -0.1501,  0.5114],\n",
       "                         [-0.2757, -0.2689,  0.9382],\n",
       "                         [-0.3410, -0.3932,  1.2996],\n",
       "                         [-0.4401, -0.5728,  1.2863]]], dtype=torch.float64)\n",
       "            hmd_info: tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "                         [ 0.3616, -0.3907,  0.0190],\n",
       "                         [-0.2730, -0.3452,  0.0263]]], dtype=torch.float64)\n",
       "            keypoint_weights: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
       "            keypoints: tensor([[[131.5654, 195.3878],\n",
       "                         [168.1244, 173.3614],\n",
       "                         [183.4056, 129.6092],\n",
       "                         [205.5105,  94.6622],\n",
       "                         [ 87.7880, 185.0687],\n",
       "                         [ 72.6694, 139.7463],\n",
       "                         [ 67.3106,  99.5323],\n",
       "                         [132.0682, 134.3755],\n",
       "                         [130.5725, 124.2660],\n",
       "                         [129.4179, 121.1424],\n",
       "                         [131.2215, 111.4049],\n",
       "                         [112.1605, 138.2046],\n",
       "                         [114.1684, 128.4338],\n",
       "                         [115.3699, 123.4191],\n",
       "                         [110.9088, 115.0091]]], dtype=torch.float64)\n",
       "        ) at 0x1d52eaead30>\n",
       ") at 0x1d52eaea100>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[31999]['data_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_list = []\n",
    "# for i in range(31999):\t\n",
    "# \timg_id = train_dataset[i]['data_samples'].img_id\n",
    "# \tif img_id in temp_list:\n",
    "# \t\tprint(img_id)\n",
    "# \t\tprint(temp_list)\n",
    "# \telse:\n",
    "# \t\ttemp_list.append(img_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
