{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import logging\n",
    "import mimetypes\n",
    "import os\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import cv2\n",
    "import json_tricks as json\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "from mmengine.logging import print_log\n",
    "\n",
    "from mmpose.apis import inference_topdown\n",
    "from mmpose.apis import init_model as init_pose_estimator\n",
    "from mmpose.evaluation.functional import nms\n",
    "from mmpose.registry import VISUALIZERS\n",
    "from mmpose.structures import merge_data_samples, split_instances\n",
    "from mmpose.utils import adapt_mmdet_pipeline\n",
    "\n",
    "try:\n",
    "\tfrom mmdet.apis import inference_detector, init_detector\n",
    "\thas_mmdet = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "\thas_mmdet = False\n",
    "\n",
    "\n",
    "from mmengine.config import Config, ConfigDict\n",
    "from typing import Union\n",
    "from mmcv.transforms import Compose\n",
    "from typing import Optional, Sequence, Union\n",
    "from mmcv.ops import RoIPool\n",
    "\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = r'C:\\Users\\user\\Documents\\GitHub\\mmpose\\my_code\\work_dirs\\HMD_mo2cap2_\\HMD_mo2cap2_config.py'\n",
    "model_ckpt = r'C:\\Users\\user\\Documents\\GitHub\\mmpose\\my_code\\work_dirs\\HMD_mo2cap2_\\epoch_20.pth'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: C:\\Users\\user\\Documents\\GitHub\\mmpose\\my_code\\work_dirs\\HMD_mo2cap2_\\epoch_20.pth\n"
     ]
    }
   ],
   "source": [
    "# build pose estimator\n",
    "pose_estimator = init_pose_estimator(\n",
    "\tmodel_config,\n",
    "\tmodel_ckpt,\n",
    "\tdevice=device,\n",
    "\t# cfg_options=dict(\n",
    "\t# \tmodel=dict(test_cfg=dict(output_heatmaps=args.draw_heatmap)))\n",
    "\t)\n",
    "visualizer = VISUALIZERS.build(pose_estimator.cfg.visualizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config (path: C:\\Users\\user\\Documents\\GitHub\\mmpose\\my_code\\work_dirs\\HMD_mo2cap2_\\HMD_mo2cap2_config.py): {'ann_file_test': 'F:\\\\mo2cap2_data_half\\\\TestSet', 'ann_file_train': 'F:\\\\mo2cap2_data_half\\\\TrainSet', 'ann_file_val': 'F:\\\\mo2cap2_data_half\\\\ValSet', 'auto_scale_lr': {'base_batch_size': 256}, 'backend_args': {'backend': 'local'}, 'codec': {'heatmap_size': (64, 64), 'input_size': (256, 256), 'sigma': 2, 'type': 'Custom_mo2cap2_MSRAHeatmap'}, 'custom_hooks': [{'type': 'SyncBuffersHook'}], 'data_mode': 'topdown', 'data_root': 'C:\\\\Users\\\\user\\\\Documents\\\\GitHub\\\\mmpose\\\\data', 'dataset_mo2cap2_test': {'ann_file': 'F:\\\\mo2cap2_data_half\\\\TestSet', 'data_mode': 'topdown', 'filter_cfg': {'filter_empty_gt': False, 'min_size': 32}, 'pipeline': [{'type': 'LoadImage'}, {'padding': 1.0, 'type': 'GetBBoxCenterScale'}, {'input_size': (256, 256), 'type': 'TopdownAffine'}, {'type': 'PackPoseInputs'}], 'test_mode': True, 'type': 'Mo2Cap2CocoDataset'}, 'dataset_mo2cap2_train': {'ann_file': 'F:\\\\mo2cap2_data_half\\\\TrainSet', 'data_mode': 'topdown', 'filter_cfg': {'filter_empty_gt': False, 'min_size': 32}, 'pipeline': [{'type': 'LoadImage'}, {'padding': 1.0, 'type': 'GetBBoxCenterScale'}, {'direction': 'horizontal', 'type': 'RandomFlip'}, {'input_size': (256, 256), 'type': 'TopdownAffine'}, {'encoder': {'heatmap_size': (64, 64), 'input_size': (256, 256), 'sigma': 2, 'type': 'Custom_mo2cap2_MSRAHeatmap'}, 'type': 'GenerateTarget'}, {'type': 'PackPoseInputs'}], 'type': 'Mo2Cap2CocoDataset'}, 'dataset_mo2cap2_val': {'ann_file': 'F:\\\\mo2cap2_data_half\\\\ValSet', 'data_mode': 'topdown', 'filter_cfg': {'filter_empty_gt': False, 'min_size': 32}, 'pipeline': [{'type': 'LoadImage'}, {'padding': 1.0, 'type': 'GetBBoxCenterScale'}, {'input_size': (256, 256), 'type': 'TopdownAffine'}, {'encoder': {'heatmap_size': (64, 64), 'input_size': (256, 256), 'sigma': 2, 'type': 'Custom_mo2cap2_MSRAHeatmap'}, 'type': 'GenerateTarget'}, {'type': 'PackPoseInputs'}], 'test_mode': False, 'type': 'Mo2Cap2CocoDataset'}, 'dataset_type': 'Mo2Cap2CocoDataset', 'default_hooks': {'checkpoint': {'interval': 10, 'max_keep_ckpts': 3, 'type': 'CheckpointHook'}, 'visualization': {'enable': True, 'interval': 15, 'kpt_thr': 0.3, 'type': 'PoseVisualizationHook'}}, 'default_scope': 'mmpose', 'env_cfg': {'cudnn_benchmark': False, 'dist_cfg': {'backend': 'nccl'}, 'mp_cfg': {'mp_start_method': 'fork', 'opencv_num_threads': 0}}, 'load_from': None, 'log_level': 'INFO', 'log_processor': {'by_epoch': True, 'num_digits': 6, 'type': 'LogProcessor', 'window_size': 50}, 'model': {'backbone': {'depth': 50, 'init_cfg': None, 'type': 'ResNet'}, 'data_preprocessor': {'bgr_to_rgb': True, 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'type': 'PoseDataPreprocessor'}, 'head': {'decoder': {'heatmap_size': (64, 64), 'input_size': (256, 256), 'sigma': 2, 'type': 'Custom_mo2cap2_MSRAHeatmap'}, 'in_channels': 2048, 'loss': {'loss_weight': 1000, 'type': 'KeypointMSELoss', 'use_target_weight': True}, 'loss_3d': {'loss_weight': 10, 'type': 'MSELoss', 'use_target_weight': False}, 'out_channels': 15, 'type': 'CustomMo2Cap2HeatmapHead'}, 'test_cfg': {'flip_mode': 'heatmap', 'flip_test': True, 'output_heatmaps': True, 'shift_heatmap': True}, 'type': 'TopdownPoseEstimator', 'train_cfg': None}, 'optim_wrapper': {'optimizer': {'lr': 0.0005, 'type': 'Adam'}}, 'param_scheduler': [{'begin': 0, 'by_epoch': False, 'end': 500, 'start_factor': 0.001, 'type': 'LinearLR'}, {'begin': 0, 'by_epoch': True, 'end': 210, 'gamma': 0.1, 'milestones': [170, 200], 'type': 'MultiStepLR'}], 'resume': False, 'test_cfg': {}, 'test_dataloader': {'batch_size': 32, 'dataset': {'ann_file': 'F:\\\\mo2cap2_data_half\\\\TestSet', 'data_mode': 'topdown', 'filter_cfg': {'filter_empty_gt': False, 'min_size': 32}, 'pipeline': [{'type': 'LoadImage'}, {'padding': 1.0, 'type': 'GetBBoxCenterScale'}, {'input_size': (256, 256), 'type': 'TopdownAffine'}, {'type': 'PackPoseInputs'}], 'test_mode': True, 'type': 'Mo2Cap2CocoDataset'}, 'drop_last': False, 'num_workers': 4, 'persistent_workers': False, 'pin_memory': True, 'sampler': {'round_up': False, 'shuffle': True, 'type': 'DefaultSampler'}}, 'test_evaluator': {'ann_file': None, 'type': 'CustomMo2Cap2Metric', 'use_action': True}, 'test_pipeline': [{'type': 'LoadImage'}, {'padding': 1.0, 'type': 'GetBBoxCenterScale'}, {'input_size': (256, 256), 'type': 'TopdownAffine'}, {'type': 'PackPoseInputs'}], 'train_cfg': {'dynamic_intervals': [(280, 1)], 'max_epochs': 20, 'type': 'EpochBasedTrainLoop', 'val_interval': 1}, 'train_dataloader': {'batch_size': 64, 'dataset': {'ann_file': 'F:\\\\mo2cap2_data_half\\\\TrainSet', 'data_mode': 'topdown', 'filter_cfg': {'filter_empty_gt': False, 'min_size': 32}, 'pipeline': [{'type': 'LoadImage'}, {'padding': 1.0, 'type': 'GetBBoxCenterScale'}, {'direction': 'horizontal', 'type': 'RandomFlip'}, {'input_size': (256, 256), 'type': 'TopdownAffine'}, {'encoder': {'heatmap_size': (64, 64), 'input_size': (256, 256), 'sigma': 2, 'type': 'Custom_mo2cap2_MSRAHeatmap'}, 'type': 'GenerateTarget'}, {'type': 'PackPoseInputs'}], 'type': 'Mo2Cap2CocoDataset'}, 'drop_last': True, 'num_workers': 4, 'persistent_workers': False, 'pin_memory': True, 'sampler': {'shuffle': True, 'type': 'DefaultSampler'}}, 'train_pipeline': [{'type': 'LoadImage'}, {'padding': 1.0, 'type': 'GetBBoxCenterScale'}, {'direction': 'horizontal', 'type': 'RandomFlip'}, {'input_size': (256, 256), 'type': 'TopdownAffine'}, {'encoder': {'heatmap_size': (64, 64), 'input_size': (256, 256), 'sigma': 2, 'type': 'Custom_mo2cap2_MSRAHeatmap'}, 'type': 'GenerateTarget'}, {'type': 'PackPoseInputs'}], 'val_cfg': {}, 'val_dataloader': {'batch_size': 32, 'dataset': {'ann_file': 'F:\\\\mo2cap2_data_half\\\\ValSet', 'data_mode': 'topdown', 'filter_cfg': {'filter_empty_gt': False, 'min_size': 32}, 'pipeline': [{'type': 'LoadImage'}, {'padding': 1.0, 'type': 'GetBBoxCenterScale'}, {'input_size': (256, 256), 'type': 'TopdownAffine'}, {'encoder': {'heatmap_size': (64, 64), 'input_size': (256, 256), 'sigma': 2, 'type': 'Custom_mo2cap2_MSRAHeatmap'}, 'type': 'GenerateTarget'}, {'type': 'PackPoseInputs'}], 'test_mode': False, 'type': 'Mo2Cap2CocoDataset'}, 'drop_last': False, 'num_workers': 4, 'persistent_workers': False, 'pin_memory': True, 'sampler': {'round_up': False, 'shuffle': True, 'type': 'DefaultSampler'}}, 'val_evaluator': {'ann_file': None, 'type': 'CustomMo2Cap2Metric', 'use_action': False}, 'val_pipeline': [{'type': 'LoadImage'}, {'padding': 1.0, 'type': 'GetBBoxCenterScale'}, {'input_size': (256, 256), 'type': 'TopdownAffine'}, {'encoder': {'heatmap_size': (64, 64), 'input_size': (256, 256), 'sigma': 2, 'type': 'Custom_mo2cap2_MSRAHeatmap'}, 'type': 'GenerateTarget'}, {'type': 'PackPoseInputs'}], 'vis_backends': [{'type': 'LocalVisBackend'}, {'init_kwargs': {'project': 'mmpose_mo2cap2dataset'}, 'type': 'WandbVisBackend'}], 'visualizer': {'name': 'visualizer', 'type': 'CustomPose3dLocalVisualizer', 'vis_backends': [{'type': 'LocalVisBackend', 'save_dir': None}, {'init_kwargs': {'project': 'mmpose_mo2cap2dataset'}, 'type': 'WandbVisBackend', 'save_dir': None}]}, 'work_dir': 'work_dirs/HMD_mo2cap2_'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_estimator.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.set_dataset_meta(\n",
    "\tpose_estimator.dataset_meta, skeleton_style='mo2cap2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "\t'det_config': 'path/to/det_config',  # detection 설정 파일 경로\n",
    "\t'det_checkpoint': 'path/to/det_checkpoint',  # detection 체크포인트 파일 경로\n",
    "\t'pose_config': 'path/to/pose_config',  # 포즈 설정 파일 경로\n",
    "\t'pose_checkpoint': 'path/to/pose_checkpoint',  # 포즈 체크포인트 파일 경로\n",
    "\t'input': '',  # 이미지/비디오 파일 경로\n",
    "\t'show': False,  # 이미지를 표시할지 여부\n",
    "\t'output_root': '',  # 출력 이미지 파일 루트 경로\n",
    "\t'save_predictions': False,  # 예측 결과를 저장할지 여부\n",
    "\t'device': 'cuda:0',  # 추론에 사용할 장치\n",
    "\t'det_cat_id': 0,  # 바운딩 박스 탐지 모델의 카테고리 ID\n",
    "\t'bbox_thr': 0.3,  # 바운딩 박스 점수 임계값\n",
    "\t'nms_thr': 0.3,  # 바운딩 박스 NMS를 위한 IoU 임계값\n",
    "\t'kpt_thr': 0.3,  # 키포인트 시각화 임계값\n",
    "\t'draw_heatmap': False,  # 모델이 예측한 히트맵을 그릴지 여부\n",
    "\t'show_kpt_idx': False,  # 키포인트의 인덱스를 표시할지 여부\n",
    "\t'skeleton_style': 'mmpose',  # 스켈레톤 스타일 선택\n",
    "\t'radius': 3,  # 시각화할 키포인트 반경\n",
    "\t'thickness': 1,  # 시각화할 링크 두께\n",
    "\t'show_interval': 0,  # 프레임당 대기 시간 (초)\n",
    "\t'alpha': 0.8,  # 바운딩 박스의 투명도\n",
    "\t'draw_bbox': False,  # 인스턴스의 바운딩 박스를 그릴지 여부\n",
    "\t'Visualizer':'CustomPose3dLocalVisualizer',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'LoadImage'},\n",
       " {'padding': 1.0, 'type': 'GetBBoxCenterScale'},\n",
       " {'input_size': (256, 256), 'type': 'TopdownAffine'},\n",
       " {'type': 'PackPoseInputs'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_test_pipeline_cfg(cfg: Union[str, ConfigDict]) -> ConfigDict:\n",
    "    \"\"\"Get the test dataset pipeline from entire config.\n",
    "\n",
    "    Args:\n",
    "        cfg (str or :obj:`ConfigDict`): the entire config. Can be a config\n",
    "            file or a ``ConfigDict``.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`ConfigDict`: the config of test dataset.\n",
    "    \"\"\"\n",
    "    if isinstance(cfg, str):\n",
    "        cfg = Config.fromfile(cfg)\n",
    "\n",
    "    def _get_test_pipeline_cfg(dataset_cfg):\n",
    "        if 'pipeline' in dataset_cfg:\n",
    "            return dataset_cfg.pipeline\n",
    "        # handle dataset wrapper\n",
    "        elif 'dataset' in dataset_cfg:\n",
    "            return _get_test_pipeline_cfg(dataset_cfg.dataset)\n",
    "        # handle dataset wrappers like ConcatDataset\n",
    "        elif 'datasets' in dataset_cfg:\n",
    "            return _get_test_pipeline_cfg(dataset_cfg.datasets[0])\n",
    "\n",
    "        raise RuntimeError('Cannot find `pipeline` in `test_dataloader`')\n",
    "\n",
    "    return _get_test_pipeline_cfg(cfg.test_dataloader.dataset)\n",
    "\n",
    "get_test_pipeline_cfg(pose_estimator.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImagesType = Union[str, np.ndarray, Sequence[str], Sequence[np.ndarray]]\n",
    "\n",
    "def inference_detector(\n",
    "    # model: nn.Module,\n",
    "    imgs: ImagesType,\n",
    "    test_pipeline: Optional[Compose] = None,\n",
    "    text_prompt: Optional[str] = None,\n",
    "    custom_entities: bool = False,\n",
    "):\n",
    "    \"\"\"Inference image(s) with the detector.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The loaded detector.\n",
    "        imgs (str, ndarray, Sequence[str/ndarray]):\n",
    "           Either image files or loaded images.\n",
    "        test_pipeline (:obj:`Compose`): Test pipeline.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`DetDataSample` or list[:obj:`DetDataSample`]:\n",
    "        If imgs is a list or tuple, the same length list type results\n",
    "        will be returned, otherwise return the detection results directly.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(imgs, (list, tuple)):\n",
    "        is_batch = True\n",
    "    else:\n",
    "        imgs = [imgs]\n",
    "        is_batch = False\n",
    "\n",
    "    # cfg = model.cfg\n",
    "\n",
    "    if test_pipeline is None:\n",
    "        # cfg = cfg.copy()\n",
    "        test_pipeline = get_test_pipeline_cfg(pose_estimator.cfg)\n",
    "        if isinstance(imgs[0], np.ndarray):\n",
    "            # Calling this method across libraries will result\n",
    "            # in module unregistered error if not prefixed with mmdet.\n",
    "            test_pipeline[0].type = 'mmdet.LoadImageFromNDArray'\n",
    "\n",
    "        test_pipeline = Compose(test_pipeline)\n",
    "\n",
    "    # if model.data_preprocessor.device.type == 'cpu':\n",
    "    #     for m in model.modules():\n",
    "    #         assert not isinstance(\n",
    "    #             m, RoIPool\n",
    "    #         ), 'CPU inference with RoIPool is not supported currently.'\n",
    "\n",
    "    result_list = []\n",
    "    for i, img in enumerate(imgs):\n",
    "        # prepare data\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # TODO: remove img_id.\n",
    "            data_ = dict(img=img, img_id=0)\n",
    "        else:\n",
    "            # TODO: remove img_id.\n",
    "            data_ = dict(img_path=img, img_id=0)\n",
    "\n",
    "        if text_prompt:\n",
    "            data_['text'] = text_prompt\n",
    "            data_['custom_entities'] = custom_entities\n",
    "\n",
    "        # build the data pipeline\n",
    "        data_ = test_pipeline(data_)\n",
    "\n",
    "        data_['inputs'] = [data_['inputs']]\n",
    "        data_['data_samples'] = [data_['data_samples']]\n",
    "\n",
    "        # forward the model\n",
    "        # with torch.no_grad():\n",
    "        #     results = model.test_step(data_)[0]\n",
    "\n",
    "        # result_list.append(results)\n",
    "\n",
    "    if not is_batch:\n",
    "        return result_list[0]\n",
    "    else:\n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_one_image(\n",
    "\t\t\t\t\targs,\n",
    "\t\t\t\t\timg,\n",
    "\t\t\t\t\t# detector,\n",
    "\t\t\t\t\tpose_estimator,\n",
    "\t\t\t\t\tvisualizer=None,\n",
    "\t\t\t\t\tshow_interval=0\n",
    "\t\t\t\t\t):\n",
    "\t\"\"\"Visualize predicted keypoints (and heatmaps) of one image.\"\"\"\n",
    "\n",
    "\t# predict bbox\n",
    "\t# det_result = inference_detector(img)\n",
    "\t# pred_instance = det_result.pred_instances.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\t# img = Image.open(img)\n",
    "\t# w,h = img.size\n",
    "\n",
    "\t# bboxes = [0,0,w,h]\n",
    "\n",
    "\t# bboxes = np.concatenate(\n",
    "\t# \t(pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
    "\t# bboxes = bboxes[np.logical_and(pred_instance.labels == args['det_cat_id'],\n",
    "\t# \t\t\t\t\t\t\t   pred_instance.scores > args['bbox_thr'])]\n",
    "\t# bboxes = bboxes[nms(bboxes, args['nms_thr']), :4]\n",
    "\t'''\n",
    "\tbbox_format (str): The bbox format indicator. Options are ``'xywh'``\n",
    "\t\tand ``'xyxy'``. Defaults to ``'xyxy'``\n",
    "\t'''\n",
    "\t# predict keypoints\n",
    "\tpose_results = inference_topdown(pose_estimator, img)\n",
    "\tdata_samples = merge_data_samples(pose_results)\n",
    "\n",
    "\t# show the results\n",
    "\tif isinstance(img, str):\n",
    "\t\timg = mmcv.imread(img, channel_order='rgb')\n",
    "\telif isinstance(img, np.ndarray):\n",
    "\t\timg = mmcv.bgr2rgb(img)\n",
    "\n",
    "\tif visualizer is not None:\n",
    "\t\tvisualizer.add_datasample(\n",
    "\t\t\t'result',\n",
    "\t\t\timage = img,\n",
    "\t\t\tdata_sample = data_samples,\n",
    "\t\t\tdraw_gt = False,\n",
    "\t\t\tdraw_pred = True,\n",
    "\t\t\tdraw_2d = True,\n",
    "\t\t\tdraw_bbox = False,\n",
    "\t\t\tshow_kpt_idx = False,\n",
    "\t\t\tskeleton_style = 'mmpose',\n",
    "\t\t\tdataset_2d = 'coco',\n",
    "\t\t\tdataset_3d = 'coco',\n",
    "\t\t\tconvert_keypoint = False,\n",
    "\t\t\taxis_azimuth = 90,\n",
    "\t\t\taxis_limit = 1.7,\n",
    "\t\t\taxis_dist = 8.0,\n",
    "\t\t\taxis_elev = -10.0,\n",
    "\t\t\tnum_instances = -1,\n",
    "\t\t\tshow = True,\n",
    "\t\t\twait_time = 0,\n",
    "\t\t\tout_file = None,\n",
    "\t\t\tkpt_thr = 0.3,\n",
    "\t\t\tstep = 0) \n",
    "\n",
    "\t# if there is no instance detected, return None\n",
    "\treturn data_samples.get('pred_instances', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n",
      "-10.0 90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<InstanceData(\n",
       "\n",
       "    META INFORMATION\n",
       "\n",
       "    DATA FIELDS\n",
       "    keypoints: array([[[695., 857.],\n",
       "                [775., 787.],\n",
       "                [535., 667.],\n",
       "                [695., 387.],\n",
       "                [555., 787.],\n",
       "                [775., 597.],\n",
       "                [435., 897.],\n",
       "                [745., 667.],\n",
       "                [745., 447.],\n",
       "                [745., 417.],\n",
       "                [765., 367.],\n",
       "                [635., 667.],\n",
       "                [615., 467.],\n",
       "                [645., 417.],\n",
       "                [585., 377.]]], dtype=float32)\n",
       "    keypoint_3d: tensor([[[ 0.3853, -0.1325,  0.5136],\n",
       "                 [ 0.6959, -0.5550,  0.4982],\n",
       "                 [-0.1153, -0.5721,  0.8959],\n",
       "                 [-0.5007, -0.2751,  0.6640],\n",
       "                 [-0.0473,  0.2331,  0.8575],\n",
       "                 [ 0.2925, -0.3798,  0.6429],\n",
       "                 [-0.1900,  0.2078,  0.4563],\n",
       "                 [ 0.0612, -0.1395,  1.0531],\n",
       "                 [-0.9025,  0.6286,  0.7685],\n",
       "                 [-1.6774,  0.9476,  0.5348],\n",
       "                 [-1.6723,  0.8985,  0.8542],\n",
       "                 [ 0.2602, -0.0556,  0.7096],\n",
       "                 [-0.7679,  0.9330,  0.6675],\n",
       "                 [-1.6726,  1.1206,  0.1146],\n",
       "                 [-1.5229,  1.1025,  0.2296]]], device='cuda:0')\n",
       "    bbox_scores: array([1.], dtype=float32)\n",
       "    bboxes: array([[   0.,    0., 1280., 1024.]], dtype=float32)\n",
       "    keypoints_visible: array([[0.21420303, 0.22793743, 0.05671426, 0.07814483, 0.17258874,\n",
       "                0.06047165, 0.11805785, 0.8488411 , 0.9298669 , 0.90776074,\n",
       "                0.9165375 , 0.93011177, 0.84686136, 0.8962437 , 0.8649585 ]],\n",
       "              dtype=float32)\n",
       "    keypoint_scores: array([[0.21420303, 0.22793743, 0.05671426, 0.07814483, 0.17258874,\n",
       "                0.06047165, 0.11805785, 0.8488411 , 0.9298669 , 0.90776074,\n",
       "                0.9165375 , 0.93011177, 0.84686136, 0.8962437 , 0.8649585 ]],\n",
       "              dtype=float32)\n",
       ") at 0x189f74d48b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_path = r'F:\\mo2cap2_data_half\\TestSet\\weipeng_studio\\rgba\\frame_c_0_f_0779.png'\n",
    "train_img_path = r'F:\\mo2cap2_data_half\\ValSet\\mo2cap2_chunk_0033\\rgba\\mo2cap2_chunk_0033_000823.png'\n",
    "# inference\n",
    "pred_instances = process_one_image(args,test_img_path,pose_estimator, visualizer)\n",
    "pred_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if args.save_predictions:\n",
    "\tpred_instances_list = split_instances(pred_instances)\n",
    "\n",
    "if output_file:\n",
    "\timg_vis = visualizer.get_image()\n",
    "\tmmcv.imwrite(mmcv.rgb2bgr(img_vis), output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
